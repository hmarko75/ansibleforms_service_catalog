---
- name: create prod volume  
  hosts: "localhost"
  become: no 
  gather_facts: no
  vars:
    cluster: "{{ vars.prod.cluster }}"
    aggregate: "{{ vars.prod.aggregate }}"
    svm: "{{ vars.prod.svm }}"
    volname: "{{ vars.prod.volname }}"
    secstyle: "{{ vars.prod.secstyle | default(VolumeDefaults.secstyle) | default(omit) }}"
    snapshot_policy: "{{ vars.prod.snapshotpolicy }}"
    percent_snapshot_space: "{{ vars.prod.snapshotpolicy.percent_snapshot_space|default(VolumeDefaults.percent_snapshot_space)|default(0)|int }}"
    volsize: "{{ vars.prod.volsize|int + vars.prod.volsize|int*percent_snapshot_space|int/100|int }}"
    cifsaccess:  "{{ vars.prod.cifsaccess }}"
    

    clusterauth:  &login
      hostname: "{{ cluster }}"
      username: "{{ login.user }}"
      password: "{{ login.password }}"
      https: true
      validate_certs: false
  
  vars_files:
  #- volume_variables.yaml
  - ../globals.yaml

  collections:
  - netapp.ontap  
  tasks:

    #generate random number for dynaic schedules and policies 
    - name: calculate base suffix for dynamic snapshot policy 
      no_log: true
      set_fact:
        #base_schedule_suffix: "{{ VolumeDefaults.dynamic.prod_snapshot_policy_range_end | random(start=VolumeDefaults.dynamic.prod_snapshot_policy_range_start) }}"
        base_schedule_suffix: "{{VolumeDefaults.dynamic.prod_snapshot_policy_range_end|random(start=VolumeDefaults.dynamic.prod_snapshot_policy_range_start) if VolumeDefaults.dynamic.prod_snapshot_policy_range_start<=VolumeDefaults.dynamic.prod_snapshot_policy_range_end else (VolumeDefaults.dynamic.total_schedules|random(start=VolumeDefaults.dynamic.prod_snapshot_policy_range_start) if (1|random(start=0))==1 else VolumeDefaults.dynamic.prod_snapshot_policy_range_end|random(start=1))}}"
      when: 
        vars.prod.snapshotpolicy.dynamicschedule == true

    #add suffix to snapshot policy name and schedules when dynamic from range
    - name: dynamic snapshot policy
      no_log: true
      set_fact:
        snapshot_policy: "{{ vars.prod.snapshotpolicy.name + (base_schedule_suffix|string) }}"
        suffixedschedules: "{{ vars.prod.snapshotpolicy.schedule | product([(base_schedule_suffix|string)]) | map('join') | list }}"
      when: 
        vars.prod.snapshotpolicy.dynamicschedule == true and vars.prod.snapshotpolicy.name not in ['none',''] 

    #add suffix to eff policy name and schedules with offset from the prod snapshot 
    - name: dynamic efficiency policy
      no_log: true
      when: 
        vars.prod.efficiency.dynamicschedule == true and vars.prod.efficiency.schedule not in ['none','']
      set_fact:
        efficiency_policy_suffix: "{{ (base_schedule_suffix|int + VolumeDefaults.dynamic.prod_efficiency_policy_offset|int)}}"

    #when suffix is < 1 rotate 
    - name: dynamic efficiency policy1
      no_log: true
      when: 
        efficiency_policy_suffix is defined  and efficiency_policy_suffix|int < 1
      set_fact:
        efficiency_policy_suffix: "{{ VolumeDefaults.dynamic.total_schedules|int + efficiency_policy_suffix|int }}"
    
    # when suffix is > from VolumeDefaults.dynamic.total_schedules
    - name: dynamic efficiency policy2
      no_log: true
      when: 
        efficiency_policy_suffix is defined  and efficiency_policy_suffix|int > VolumeDefaults.dynamic.total_schedules
      set_fact:
        efficiency_policy_suffix: "{{ efficiency_policy_suffix|int - VolumeDefaults.dynamic.total_schedules|int }}"

    # log volume infomration 
    - name: vol creation info
      ansible.builtin.debug:
        msg:
        - "creating prod volume: {{ cluster }}:{{ svm }}:{{ volname }} size:{{ volsize }}GB copy:{{ copy }} snapshot policy:{{ snapshot_policy }}"  

    #create efficiency policy 
    - name: Create Scheduled efficiency Policy
      na_ontap_efficiency_policy:
        state: present
        vserver: "{{ svm }}"
        policy_name: "{{ vars.prod.efficiency.schedule + efficiency_policy_suffix if efficiency_policy_suffix is defined else vars.prod.efficiency.schedule }}"
        enabled: true
        schedule: "{{ vars.prod.efficiency.schedule + efficiency_policy_suffix if efficiency_policy_suffix is defined else vars.prod.efficiency.schedule }}"
        duration: "{{ vars.prod.efficiency.duration | default(VolumeDefaults.dynamic.prod_efficiency_policy_duration) | default(omit) }}"
        qos_policy: background
        <<: *login

    #create snapshot policy 
    - name: Create Prod Snapshot Policy
      na_ontap_snapshot_policy:
        state: present
        vserver: "{{ svm }}"
        name: "{{ snapshot_policy }}"
        schedule: "{{ suffixedschedules if suffixedschedules is defined else vars.prod.snapshotpolicy.schedule }}"
        prefix: "{{ vars.prod.snapshotpolicy.prefix }}"
        count: "{{ vars.prod.snapshotpolicy.count }}"
        snapmirror_label: "{{ vars.prod.snapshotpolicy.snapmirror_label }}"
        enabled: True
        <<: *login

    - name: Create Prod Export Policy
      na_ontap_export_policy:
        state: present
        name: "{{ volname }}"
        vserver: "{{ svm }}" 
        <<: *login    

    - name: Create Prod ExportPolicyRule
      netapp.ontap.na_ontap_export_policy_rule:
        state: present
        name: "{{ volname }}"
        vserver: "{{ svm }}" 
        client_match: "{{ vars.prod.exports }}"
        ro_rule: sys
        rw_rule: sys
        protocol: nfs
        super_user_security: any
        <<: *login
      when: 
        vars.prod.exports != ''

    - name: Create Prod Volume
      na_ontap_volume:
        state: present
        aggregate_name: "{{ aggregate }}"
        name: "{{ volname }}"
        vserver: "{{ svm }}"
        size: "{{ volsize|int }}"
        junction_path: "{{ VolumeDefaults.default_junction_path + volname if VolumeDefaults.default_junction_path is defined else '/'+volname }}"
        snapshot_policy: "{{ snapshot_policy | default(omit) }}"
        percent_snapshot_space: "{{ percent_snapshot_space }}"
        volume_security_style: "{{ secstyle | default(omit) }}"
        export_policy: "{{ volname }}"
        comment: "{{ general.comment | default(omit)}}"
        <<: *login

    - name: Enable Volume efficiency
      na_ontap_volume_efficiency:
        state: present
        vserver: "{{ svm }}"
        path: "/vol/{{ volname }}"
        enable_compression: "{{ vars.prod.efficiency.enable_compression | default(omit) }}"
        enable_cross_volume_background_dedupe: "{{  vars.prod.efficiency.enable_cross_volume_background_dedupe | default(omit) }}"
        enable_cross_volume_inline_dedupe: "{{ vars.prod.efficiency.enable_cross_volume_inline_dedupe | default(omit) }}"
        enable_data_compaction: "{{ vars.prod.efficiency.enable_data_compaction | default(omit) }}"
        enable_inline_compression: "{{ vars.prod.efficiency.enable_inline_compression | default(omit) }}"
        enable_inline_dedupe: "{{ vars.prod.efficiency.enable_inline_dedupe | default(omit) }}"
        #not realy working to set policy 
        #policy: "{{ vars.prod.efficiency.schedule + efficiency_policy_suffix if efficiency_policy_suffix is defined else vars.prod.efficiency.schedule }}"
        <<: *login
    
    #set efficiency policy using ssh due to issue with na_ontap_volume_efficiency and effciency policy 
    - name: Set volume efficiency policy
      na_ontap_ssh_command:
        hostname: "{{ cluster }}"
        username: "{{ login.user }}"
        password: "{{ login.password }}"
        command: "{{ 'vol efficiency modify -vserver '+svm+' -volumme '+volname+' -policy '+(vars.prod.efficiency.schedule + efficiency_policy_suffix if efficiency_policy_suffix is defined else vars.prod.efficiency.schedule) }}"
        accept_unknown_host_keys: true
        privilege: diag
      when: 
        vars.prod.efficiency.schedule is defined
      
    - name: Modify volume autosize
      na_ontap_volume_autosize:
        vserver: "{{ svm }}"
        volume: "{{ volname }}"
        mode: "{{ VolumeDefaults.auto_size[copy].mode | default(omit) }}"
        grow_threshold_percent: "{{ VolumeDefaults.auto_size[copy].grow_threshold_percent | default(omit) }}"
        shrink_threshold_percent: "{{ VolumeDefaults.auto_size[copy].shrink_threshold_percent | default(omit) }}"
        maximum_size: "{{ ((VolumeDefaults.auto_size[copy].vol_max_size_ratio*(volsize|float))|int|string)+'g' if (VolumeDefaults.auto_size[copy].vol_max_size_ratio*(volsize|float)) <= VolumeDefaults.auto_size[copy].max_size else volsize|string+'g' | default(omit) }}"
        minimum_size: "{{ ((VolumeDefaults.auto_size[copy].vol_min_size_ratio*(volsize|float))|int|string)+'g' | default(omit) }}"
        <<: *login

    - name: Create CIFS share
      netapp.ontap.na_ontap_cifs:
        state: present
        share_name: "{{ volname + vars[copy].cifsaccess.share_suffix|default('') }}"
        path: "{{ VolumeDefaults.default_junction_path + volname if VolumeDefaults.default_junction_path is defined else '/'+volname }}"
        vserver: "{{ svm }}"
        <<: *login
      when:
        vars[copy].cifsaccess is defined
      ignore_errors: yes

    #Remove everyone 
    - name: Create CIFS share ACL 
      netapp.ontap.na_ontap_cifs_acl:
        state: absent
        share_name: "{{ volname + vars[copy].cifsaccess.share_suffix|default('') }}"
        vserver: "{{ svm }}"
        user_or_group: everyone
        <<: *login
      when:
        vars[copy].cifsaccess is defined 
      ignore_errors: yes
    
    #set user from volume defaults 
    - name: Create CIFS share ACL 
      netapp.ontap.na_ontap_cifs_acl:
        state: present
        share_name: "{{ volname + vars[copy].cifsaccess.share_suffix|default('') }}"
        vserver: "{{ svm }}"
        user_or_group: "{{ vars[copy].cifsaccess.user_or_group | default('everyone') }}"
        permission: "{{ vars[copy].cifsaccess.access | default('full_control') }}"
        <<: *login
      when:
        vars[copy].cifsaccess is defined 
      ignore_errors: yes

- name: Rediscover cluster in AIQUM
  import_playbook: aiqum_rediscover_cluster.yaml
  vars:
    cluster: "{{ vars[copy].cluster | default (omit)}}"
    cluster_key: "{{ vars[copy].clusterkey | default (omit)}}"
  ignore_errors: yes